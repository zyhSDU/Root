b1.数据分析
	c1.数据分析的六大步骤
		d1.明确目的和思路
			e1.先决条件、提供项目方向
		d2.数据收集
			e1.数据库建立
		d3.数据处理
			e1.清洗、转化、提取、计算
		d4.数据分析
			e1.数据统计、数据挖掘
		d5.数据展现
			e1.图标、表格、文字
		d6.报告撰写
			e1.架构清晰、明确结论、提出建议
b2.大数据的特点-4v理论
	c1.volume	数据规模大
	c2.variety	数据类型多
	c3.value	数据价值高
	c4.velocity数据处理速度快
b3.数据
	c1.数据对象和属性类型
		d1.属性类型
			e1.标称属性（名词属性）
			e2.二进制属性（0和1，真和假）
			e3.次序属性
			e4.数值型属性
			e5.间隔尺度属性
			e6.比例尺度属性
			e7.离散和连续属性
	c2.数据的基本统计描述
		d1.目的
			e1.更好地识别数据的性质，把握数据全貌:中心趋势度量，数据散布
		d2.中心趋势度量(measures of central tendency)
			e1.均值mean
				f1.（算术）平均数
					g1.对极端值敏感。可以进行削减均值，但会导致数据信息丢失
				f2.加权算术平均值
			e2.中位数
				f1.中位数
				f2.中值区间
			e3.众数
				f1.单峰值，二峰值，三峰值
			e4.中列数
				f1.中列数是数据集中最大值和最小值的平均值。可以用来评估数值型数据的中心性趋势
			e5.数据的对称和偏斜
				f1.在对称的单峰频率曲线数据分布中，平均数，中值和众数都在同样的中点值上
				f2.实际应用中，绝大部分都不是对称的。如果众数的值小于中值，称为正偏斜；如果众数的值大于中值，称为负偏斜
		d3.数据的散布(dispersion of the data)
			e1.极差
				f1.极差
					g1.最大值和最小值的差
				f2.分位点
					g1.分位点是数据分布上有规律率的间隔的数据点，将其分成相等大小的连续的数据集。
				f3.分位数
					g1.中位数，四分位数和百分位数是使用最广泛的分位数。
						h1.2-分位点是把数据分布分割成较小值和较大值两半的数据点。即中位数。
						h2.4-分位点是把数据分布分成4个等量大小的3个数据点，每一个部分表示数据分布的1/4.它们被称为四分位数。
						h3.100-分位数更通常被称为百分位数，它们将数据集分成100个大小相等的连续集合
					g2.分位数反应了分布的中心，散布以及形状。
				f4.四分位数
					g1.Q1,Q2,Q3
				f5.四分位差
					g1.Q1和Q3的距离，简单反应了数据中心的一半数据的范围。
			e2.四分位数极差
			e3.五数概括
				f1.Minimum, Q1, Median, Q3, Maximum
			e4.箱子图
				f1.描述
					g1.箱子的长度是四分位差
					g2.中值是箱子中间的线
					g3.箱子外面的两根须是观察的最小值和最小值
			e5.方差和标准差
		d4.数据可视化(graphic displays of basic statistical descriptions)
			e1.分位数图
			e2.直方图
			e3.散点图和数据相关性
	c3.衡量数据相似性和相异性
		d1.定义
			e1.相似性(Similarity)
				f1.两个对象相似程度的数量表示
				f2.数值越高表明相似性越大
				f3.通常取值范围为[0,1]
			e2.相异性(Dissimilarity)(例如距离)
				f1.两个对象不相似程度的数量表示
				f2.数值越低表明相似性越大
				f3.相异性的最小值通常为0
				f4.相异性的最大值（上限）是不同的
			e3.邻近性(Proximity):相似性和相异性都称为邻近性
		d2.数据矩阵与相异性矩阵
			e1.数据矩阵-对象-属性结构
				f1.行-对象：n个对象
				f2.列-属性：p个属性
				f3.二模矩阵(Two modes)
			e2.相异性矩阵(Dissimilarity matrix)
				f1.n个对象两两之间的邻近度
				f2.对称矩阵
				f3.单模(Single mode)
				f4.其中d(i,j)表示对象i与对象j之间的相异性
					g1.相似性度量可以表示成相异性度量的函数
					g2.Sim(i,j)=1-d(i,j)
		d3.相似性度量
			e1.标称属性的邻近性度量
			e2.二元属性的邻近性度量
			e3.数值属性的相异性:闵可夫斯基距离
				f1.h = 1:  曼哈顿距离(或城市块距离Manhattan distance)
				f2.h = 2:  欧几里德距离(用的最多的)
			e4.序数属性的邻近性度量
			e5.混合类型属性的相异性
			e6.余弦相似性
			e7.文本相似性的计算
				f1.使用TF-IDF(词频"（TF）和"逆文档频率idf)算法，找出两篇文章的关键词；
				f2.每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；
				f3.生成两篇文章各自的词频向量；
				f4.计算两个向量的余弦相似度，值越大就表示越相似。
				f5.词频 (TF) 是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。
				f6.一个计算文件频率 (IDF) 的方法是文件集里包含的文件总数除以测定有多少份文件出现过“母牛”一词。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是 lg10,000,000 / 1,000)=4。
				f7.TF-IDF的分数为0.03 * 4=0.12。
b4.数据预处理
	c1.为什么数据预处理
		d1.现实世界中的数据是脏的
			e1.不完全: 缺少属性值, 缺少某些有趣的属性, 或仅包含聚集数据
				f1.数据收集时未包含
				f2.数据收集和数据分析时的不同考虑.
				f3.人/硬件/软件问题
			e2.噪音: 包含错误或孤立点
				f1.收集
				f2.录入
				f3.变换
			e3.不一致: 编码或名字存在差异
				f1.不同的数据源
				f2.违反函数依赖
		d2.没有高质量的数据, 就没有高质量的数据分析和挖掘结果!
			e1.高质量的决策必然依赖高质量的数据
				f1.例如, 重复或遗漏的数据可能导致不正确或误导的统计.
			e2.数据仓库需要高质量数据的一致集成
	c2.数据质量：一个多维视角
		d1.一种广泛接受的多角度:
			e1.正确性(Accuracy)
			e2.完全性(Completeness)
			e3.一致性(Consistency)
			e4.合时(Timeliness)：timely update?
			e5.可信性(Believability)
			e6.可解释性(Interpretability)
			e7.可存取性(Accessibility)
	c3.主要任务
		d1.数据清理
			e1.填充缺失值, 识别/去除离群点, 光滑噪音, 并纠正数据中的不一致
			e2.如何处理缺失数据?
				f1.忽略元组: 缺少类别标签时常用(假定涉及分类)—不是很有效，当每个属性的缺失百分比变化大时
				f2.手工填写缺失数据: 乏味+费时+不可行 ?
				f3.自动填充
					g1.一个全局常量 : e.g., “unknown”, a new class?!
					g2.使用属性均值
					g3.与目标元组同一类的所有样本的属性均值: 更巧妙
					g4.最可能的值: 基于推理的方法，如贝叶斯公式或决策树
			e3.处理噪音数据的方法
				f1.分箱Binning method(考察数据的“近邻”来光滑有序数据值，这些有序的值被分布到一些“桶”或箱中)
					g1.排序数据，分布到等频/等宽的箱/桶中
					g2.箱均值光滑、箱中位数光滑、箱边界光滑, etc.
					g3.分箱：把待处理的数据按照一定的规则放进一些箱子中，考察每一个箱子中的数据，采用某种方法分别对各个箱子中的数据进行处理。
					g4.箱子：按照属性值划分的子区间，如果一个属性值处于某个子区间范围内，就称把该属性值放进这个子区间代表的“箱子”里。
					g5.如何分箱
						h1.等宽度Equal-width (distance) 剖分:
							i1.分成大小相等的n个区间: 均匀网格 uniform grid
							i2.若A和B是 属性的最低和最高取值, 区间宽度为: W = (B –A)/N.
							i3.孤立点可能占据重要影响 may dominate presentation
							i4.倾斜的数据处理不好.
						h2.等频剖分 (frequency) /等深equi-depth :
							i1.分成n个区间, 每一个含近似相同数目的样本
							i2.Good data scaling
							i3.类别属性可能会非常棘手.
					g6.数据平滑方法，即如何对每个箱子中的数据进行平滑处理
						h1.按箱平均值平滑
						h2.按箱中值平滑
						h3.按箱边界值平滑
				f2.聚类Clustering
					g1.检测和去除 离群点/孤立点 outliers
					g2.簇：一组数据对象集合。同一簇内的所有对象具有相似性，不同簇间对象具有较大差异性。
					g3.聚类：将物理的或抽象对象的集合分组为由不同簇，找出并清除那些落在簇之外的值（孤立点），这些孤立点被视为噪声。
					g4.通过聚类分析发现异常数据：相似或相邻近的数据聚合在一起形成了各个聚类集合，而那些位于这些聚类集合之外的数据对象，自然而然就被认为是异常数据。
					g5.特点：直接形成簇并对簇进行描述，不需要任何先验知识。
					g6.聚类
						h1.每个簇中的数据用其中心值代替
						h2.忽略孤立点
						h3.先通过聚类等方法找出孤立点。这些孤立点可能包含有用的信息。
						h4.人工再审查这些孤立点
				f3.回归 Regression
					g1.回归函数拟合数据
					g2.回归：发现两个相关的变量之间的变化模式，通过使数据适合一个函数来平滑数据，即利用拟合函数对数据进行平滑。
					g3.通过构造函数来符合数据变化的趋势，这样可以用一个变量预测另一个变量。
						h1.线性回归（简单回归）：利用直线建模，将一个变量看作另一个变量的线性函数
							i1.如：Y=aX+b，其中a、b称为回归系数，可用最小二乘法求得a、b系数
					g4.非线性回归
				f4.计算机和人工检查相结合
					g1.人工检查可疑值 (e.g., deal with possible outliers)
		d2.数据集成
			e1.多个数据库, 数据立方体, 或文件的集成
				f1.数据仓库就是把来自各个独立数据源的数据加载并存储到一个物理数据库（称为数据仓库）中，然后就可以在这些数据上进行查询等操作。虚拟集成系统中，数据还是保存在原来的数据源中，只在需要查询时才被访问。
			e2.为什么
				f1.集成多个来源的数据可以帮助降低和避免结果数据集中的冗余和不一致，提高数据挖掘的速度和质量
			e3.模式匹配
				f1.在中介模式和源数据模式上建立映射关系
			e4.属性冗余
				f1.同一属性在不同的数据库中会有不同的名称
				f2.一个属性可以由另外一个表导出
				f3.有些冗余可以被相关分析检测到
					g1.给定两个属性，根据可用数据，度量一个属性能在多大程度上蕴含另一个
					g2.协相关系数 (数值数据)
					g3.X^2卡方检验
						h1.Χ^2值越大,相关的可能越大
			e5.实体识别
				f1.多个数据源的真实世界的实体的识别
			e6.去重
				f1.除了检测属性间的冗余外，还应当在元组级检测重复（例如，对于给定的唯一数据实体，存在两个或多个相同的元组）。
				f2.不一致通常出现在各种不同的副本之间，由于不正确的数据输入，或者由于更新了数据的某些出现，但未更新所有的出现。
				f3.例如，如果订单数据库包含订货人的姓名和地址属性，而不是这些信息在订货人数据库中的码，则差异就可能出现，如同一订货人的名字可能以不同的地址出现在订单数据库中。
			e7.数据值冲突的检测与处理
				f1.产生的原因
					g1.表示的差异、比例尺度不同、或编码的差异等
					g2.例如：重量属性在一个系统中采用公制，而在另一个系统中却采用英制。同样价格属性不同地点采用不同货币单位。
		d3.数据归约（数据消减）
			e1.为什么
				f1.对大规模数据库内容进行复杂的数据分析通常需要耗费大量的时间。
				f2.数据归约（消减）技术用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性
				f3.这样在精简数据集上进行数据挖掘显然效率更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。
			e2.标准
				f1.用于数据归约的时间不应当超过或“抵消”在归约后的数据上挖掘节省的时间
				f2.归约得到的数据比原数据小得多，但可以产生相同或几乎相同的分析结果
			e3.策略
				f1.数据立方体聚集
					g1.数据立方体基本概念：
						h1.数据立方体是数据的多维建模和表示，由维和事实组成
						h2.维：属性
						h3.事实：数据
					g2.定义
						h1.将n维数据立方体聚集为n-1维的数据立方体
					g3.数据立方体存储多维聚集信息
						h1.某抽象层上建的数据立方体称为方体(cuboid)
						h2.最底层建的方体称为基本方体(base cuboid)
						h3.最高层的立方体称为 顶点方体(apex cuboid)
					g4.每个更高层的抽象将减少数据的规模
					g5.使用合适的抽象层上的数据
						h1.对数据立方体聚集得到与任务相关的最小立方体
					g6.聚集后数据量明显减少，但没有丢失分析任务所需的信息
				f2.维归约
					g1.去掉无关的属性，减少数据挖掘处理的数据量
					g2.维归约方法包括小波变换和主成分分析，它们把原数据变换或投影到较小的空间
					g3.属性子集选择是一种维归约方法，其中不相关、弱相关或冗余的属性或维被检测和删除。
					g4.目标
						h1.寻找出最小的属性子集并确保新数据子集的概率分布尽可能接近原来数据集的概率分布。
					g5.特征选择(i.e., 属性子集选择):
						h1.删除不相关/冗余属性，减少数据集
						h2.找出最小属性集，类别的数据分布尽可能接近使用全部属性值的原分布
						h3.减少了发现的模式数目, 容易理解
						h4.d个属性，有2d 个可能的属性子集
					g6.选择相关属性子集
						h1.启发式方法Heuristic methods (因为指数级的可能性):
							i1.逐步向前选择
								j1.从一个空属性集（作为属性子集初始值）开始，每次从原来属性集合中选择一个当前最优的属性添加到当前属性子集中。直到无法选择出最优属性或满足一定阈值约束为止。
							i2.逐步向后删除
								j1.从一个全属性集（作为属性子集初始值）开始，每次从当前属性子集中选择一个当前最差的属性并将其从当前属性子集中消去。直到无法选择出最差属性为止或满足一定阈值约束为止。
							i3.向前选择和向后删除结合
							i4.判定树（决策树）归纳
								j1.利用决策树的归纳方法对初始数据进行分类归纳学习，获得一个初始决策树，所有没有出现这个决策树上的属性均认为是无关属性，因此将这些属性从初始属性集合删除掉，就可以获得一个较优的属性子集。
							i5.基于统计分析的归约
								j1.特征独立性假设下，最好的单个属性: 统计显著性检验方法
								j2.两样本t-test，等
					g7.属性/特征产生
						h1.F产生新的属性，其可以比原始属性更有效地表示数据的重要信息。
						h2.三个一般方法:
							i1.属性提取
								j1.特定领域的
							i2.映射数据到新空间
								j1.傅立叶变换
								j2.流形方法
							i3.属性构造
								j1.组合特征
								j2.数据离散化
					g8.主成分分析
						h1.找到一个投影，其能表示数据的最大变化
						h2.原始数据投影到一个更小的空间中，导致维度减少
							i1.发现的协方差矩阵的特征向量，用这些特征向量定义新的空间
				f3.数值归约
					g1.用替代的、较小的数据表示形式替换原数据。这些技术可以是参数的或非参数的
					g2.参数方法
						h1.假设数据适合某个模型，估计模型参数，仅存储参数，并丢弃数据（孤立点除外）
						h2.回归分析：用于预测（包括时间序列数据的预测），推断，假设检验和因果关系的建模
							i1.非线性回归
							i2.线性回归
								j1.数据拟合到一条直线上
								j2.最小二乘法
							i3.多元线性回归
								j1.允许响应变量Y表示为多个预测变量的函数
							i4.对数线性模型
								j1.近似离散的多维概率分布
					g3.非参数方法：不假定模型
						h1.直方图
						h2.聚类
							i1.将对象划分成集/簇, 用簇的表示替换实际数据
								j1.技术的有效性依赖于数据的质量
							i2.使用层次聚类，并多维索引树结构存放
							i3.非常多的聚类算法和定义
						h3.抽样
							i1.获得一个小的样本集s来表示整个数据集N
							i2.关键原则
								j1.选择一个有代表性的数据子集
							i3.缺点
								j1.数据偏斜时简单随机抽样的性能很差
								j2.采样不能减少数据库I/O（一次一页）
							i4.优点
								j1.获取样本的时间仅与样本规模成正比
							i5.抽样类型
								j1.简单随机抽样
								j2.无放回抽样
								j3.放回抽样
								j4.分层抽样
									k1.把数据分成不相交部分(层), 然后从每个层抽样(按比例/大约相同比例的数据)
						h4.数据立方体聚集
				f4.数据压缩
					g1..
						h1.使用变换，以便得到原数据的归约或“压缩”表示。
						h2.无损：如果原数据能够从压缩后的数据重构，而不损失信息，
						h3.有损：如果我们只能近似重构原数据，则该数据归约称为有损的。
						h4.对于串压缩，有一些无损压缩算法。然而，它们一般只允许有限的数据操作。
						h5.维归约和数量归约也可以视为某种形式的数据压缩。
					g2.分类
						h1.字符串压缩
							i1.有丰富的理论和调优的算法
							i2.典型的是有损压缩；
							i3.但只有有限的操作是可可行的
						h2.音频/视频压缩
							i1.通常有损压缩，逐步细化
							i2.有时小片段的信号可重构，而不需要重建整个信号
						h3.时间序列不是音频
							i1.通常短，随时间缓慢变化
						h4.维度和数值规约可以被看成是数据压缩的一种形式
				f5.离散化与概念分层生成
					g1.离散化
						h1.三种类型属性
							i1.标称
							i2.序数
							i3.连续数值
						h2.离散化技术
							i1.以通过将属性（连续取值）域值范围分为若干区间，来帮助消减一个连续（取值）属性的取值个数。
						h3.离散化:把连续属性的区域分成区间
							i1.区间标号可以代替实际数据值
							i2.利用离散化减少数据量
							i3.有监督 vs. 无监督：是否使用类的信息
							i4.某个属性上可以递归离散化
							i5.分裂 Split (top-down) vs. 合并merge (bottom-up)
								j1.自顶向下：由一个/几个点开始递归划分整个属性区间
					g2.概念分层
						h1.概念分层定义了一组由低层概念集到高层概念集的映射。它允许在各种抽象级别上处理数据，从而在多个抽象层上发现知识。用较高层次的概念替换低层次（如年龄的数值）的概念，以此来减少取值个数。虽然一些细节在数据泛化过程中消失了，但这样所获得的泛化数据或许会更易于理解、更有意义。在消减后的数据集上进行数据挖掘显然效率更高。
						h2.概念分层结构可以用树来表示，树的每个节点代表一个概念。
						h3.数值数据的概念分层生成方法
							i1.分箱
								j1.属性的值可以通过将其分配到各分箱中而将其离散化。利用每个分箱的均值和中数替换每个分箱中的值（利用均值或中数进行平滑）。循环应用这些操作处理每次操作结果，就可以获得一个概念层次树。
							i2.直方图
								j1.循环应用直方图分析方法处理每次划分结果，从而最终自动获得多层次概念树，而当达到用户指定层次水平后划分结束。最小间隔大小也可以帮助控制循环过程，其中包括指定一个划分的最小宽度或每一个层次每一划分中数值个数等。
							i3.聚类
								j1.聚类算法可以将数据集划分为若干类或组。每个类构成了概念层次树的一个节点；每个类还可以进一步分解为若干子类，从而构成更低水平的层次。当然类也可以合并起来构成更高层次的概念水平。
							i4.自然划分分段
								j1.将数值区域划分为相对一致的、易于阅读的、看上去更直观或自然的区间。
								j2.聚类分析产生概念分层可能会将一个工资区间划分为：[51263.98, 60872.34]
								j3.通常数据分析人员希望看到划分的形式为[50000，60000]
								j4.划分方法：3-4-5规则
									k1.如果一个区间最高有效位上包含3，6，7或9个不同的值，就将该区间划分为3个等宽子区间；(72,3,2)
									k2.如果一个区间最高有效位上包含2，4，或8个不同的值，就将该区间划分为4个等宽子区间；
									k3.如果一个区间最高有效位上包含1，5，或10个不同的值，就将该区间划分为5个等宽子区间；
									k4.将该规则递归的应用于每个子区间，产生给定数值属性的概念分层；
									k5.对于数据集中出现的最大值和最小值的极端分布，为了避免上述方法出现的结果扭曲，可以在顶层分段时，选用一个大部分的概率空间(如 5%-95%)，越出顶层分段的特别高和特别低的采用类似的方法形成单独的区间。
							i5.基于熵的离散化
						h4.标称数据的概念分层产生
							i1.用更抽象（更高层次）的概念来取代低层次或数据层的数据对象
							i2.例如：街道属性，就可以泛化到更高层次的概念，诸如：城市、国家。同样对于数值型的属性，如年龄属性，就可以映射到更高层次概念，如：年轻、中年和老年。
			e4.规范化和聚集
		d4.数据转换
			e1.得到数据的归约表示, 它小得多, 但产生相同或类似的分析结果：维度规约、数值规约、数据压缩
			e2.数据被变换或统一成适于分析和挖掘的形式
			e3.平滑: 去掉噪音，技术：分箱、回归、聚类
			e4.聚集Aggregation:对数据进行汇总, 数据立方体构造
			e5.属性Attribute/特征feature 构造
				f1.从给定的属性构造新属性
				f2.机器学习中称为：特征构造
			e6.数据概化：用更抽象（更高层次）的概念来取代低层次或数据层的数据对象
			e7.规范化Normalization:按比例缩放到一个具体区间
				f1.最小-最大规范化
				f2.z-score 规范化
				f3.小数定标规范化
				f4.离散化：数值属性的原始值（如年龄）用区间标签（0-10,11-20等）或概念标签（如youth,adult,senior）替换，这些标签可以递归组织成更高层的概念，导致数值属性的概念分层
			e8.标称数据的概念分层
			e9.光滑
				f1.去除噪声，将连续的数据离散化，增加粒度
					g1.分箱
					g2.聚类
					g3.回归
			e:.聚集
				f1.对数据进行汇总
					g1.avg(), count(), sum(), min(), max()…
					g2.例如：每天销售额（数据）可以进行合计操作以获得每月或每年的总额。
					g3.可以用来构造数据立方体
			e;.属性构造
				f1.利用已有属性集构造出新的属性，并加入到现有属性集合中以帮助挖掘更深层次的模式知识，提高挖掘结果准确性。
				f2.例如：根据宽、高属性，可以构造一个新属性：面积。
			e<.数据概化
				f1.用更抽象（更高层次）的概念来取代低层次或数据层的数据对象
				f2.例如：街道属性，就可以泛化到更高层次的概念，诸如：城市、国家。同样对于数值型的属性，如年龄属性，就可以映射到更高层次概念，如：年轻、中年和老年。
			e=.规范化
				f1.将数据按比例进行缩放，使之落入一个特定的区域，以消除数值型属性因大小不一而造成挖掘结果的偏差。如将工资收入属性值映射到[-1.0,1.0]范围内。
				f2.方法
					g1.最小-最大规范化
						h1.保留了原来数据中存在的关系。但若将来遇到超过目前属性[old_min,old_max]取值范围的数值，将会引起系统出错
					g2.零-均值规范化（z-score规范化）
						h1.根据属性A的均值和偏差来对A进行规格化,常用于属性最大值与最小值未知；或使用最大最小规格化方法时会出现异常数据的情况。
					g3.小数定标规范化
						h1.通过移动属性A值的小数位置，将属性A的值映射到[0，1]之间，用小数的科学表示法来达到规格化的目的。
						h2.移动的小数位数取决于属性A绝对值的最大值。
			e>.属性构造
				f1.利用已有属性集构造出新的属性，并加入到现有属性集合中以帮助挖掘更深层次的模式知识，提高挖掘结果准确性。
				f2.例如：根据宽、高属性，可以构造一个新属性：面积。
	c4.污染数据形成的原因
		d1.滥用缩写词
		d2.数据输入错误
		d3.数据中的内嵌控制信息
		d4.不同的惯用语
		d5.重复记录
		d6.丢失值
		d7.拼写变化
		d8.不同的计量单位
		d9.过时的编码
		d:.含有各种噪声
